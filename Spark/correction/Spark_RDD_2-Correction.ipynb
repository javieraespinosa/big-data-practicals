{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cet atelier, nous allons explorer certains les concepts de RDD que nous avons abordés. Nous utiliserons un ensemble de données comprenant les crimes signalés à Washington, du 3 octobre 2015 au 2 octobre 2016. Ces données proviennent du catalogue de données ouvertes du district de Columbia. Nous utiliserons ces données pour explorer certaines transformations et actions sur les RDD et les pairs RDD.\n",
    "https://github.com/frankieliu/databricks2/blob/master/wash_dc_crime_incidents_2013.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Pré-requis\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Démarrer HDFS :"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "docker start namenode datanode1 datanode2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Démarrer Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour démarrer votre cluster Spark, si cela n'a pas déjà été fait, vous devrez ouvrir un terminal puis exécuter la commande suivante :"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "docker start smaster sworker1 sworker2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérifiez que les containers smaster, sworker1 et sworker2 sont démarrés :"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connexion à Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etablissez une connexion au cluster Spark :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Attention le driver ne peut instancier qu'un seul SparkContext\n",
    "# Si vous exécuter plusieurs fois cette cellule vous obtiendrez une erreur:\n",
    "# Cannot run multiple SparkContexts at once;\n",
    "# Pensez bien à fermer le Sparkcontext à la fin de chaque TP avec la commande sc.stop()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(\"spark://smaster:7077\", \"TPSPARK\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour vérifier le point d'accés au cluster : \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spark://smaster:7077'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Créer un RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La première étape consiste à charger les données. \n",
    "Exécutez la cellule suivante pour créer un RDD contenant les données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRdd =  sc.textFile(\"/dataspark/wash_dc_crime_incidents_2013.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La lecture d'un fichier n'est pas une action et par conséquent aucun traitement sera effectué.\n",
    "\n",
    "Lancer votre première action de comptage :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35898"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voyons quelques-unes des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CCN,REPORTDATETIME,SHIFT,OFFENSE,METHOD,LASTMODIFIEDDATE,BLOCKSITEADDRESS,BLOCKXCOORD,BLOCKYCOORD,WARD,ANC,DISTRICT,PSA,NEIGHBORHOODCLUSTER,BUSINESSIMPROVEMENTDISTRICT,BLOCK_GROUP,CENSUS_TRACT,VOTING_PRECINCT,START_DATE,END_DATE',\n",
       " '04104147,4/16/2013 12:00:00 AM,MIDNIGHT,HOMICIDE,KNIFE,8/7/2015 8:34:01 AM,1500 - 1599 BLOCK OF 1ST STREET SW,398943,133729,6,6D,FIRST,105,9,,006400 2,006400,Precinct 127,7/27/2004 8:30:00 PM,7/27/2004 8:30:00 PM',\n",
       " '05047867,6/5/2013 12:00:00 AM,MIDNIGHT,SEX ABUSE,KNIFE,8/7/2015 8:32:22 AM,6500  - 6599 BLOCK OF PINEY BRANCH ROAD NW,397769,144596,4,4B,FOURTH,402,17,,001901 4,001901,Precinct 59,4/15/2005 12:30:00 PM,',\n",
       " '07083463,7/8/2013 12:00:00 AM,MIDNIGHT,SEX ABUSE,OTHERS,8/7/2015 8:32:15 AM,1800 - 1810 BLOCK OF COLUMBIA ROAD NW,396275,139402,1,1C,THIRD,303,1,ADAMS MORGAN,004002 1,004002,Precinct 25,7/14/2007 3:00:00 PM,',\n",
       " '09172197,4/8/2013 12:00:00 AM,MIDNIGHT,SEX ABUSE,OTHERS,8/7/2015 8:33:35 AM,2322 - 2499 BLOCK OF ONTARIO ROAD NW,396518,139335,1,1C,THIRD,303,1,,003800 1,003800,Precinct 24,5/22/2009 1:00:00 PM,5/22/2009 3:00:00 AM']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Affichez les 5 premières lignes du RDD \"myRdd\"\n",
    "myRdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Super, comme vous pouvez le constater il y a un en-tête. \n",
    "Cet en-tête est fourni au début du fichier, cela signifie qu'une seul partition disposera de l'en-tête.\n",
    "Pour effectuer un traitement homogène sur l'ensemble des données, nous devrons vérifier si le bloc en cours de traitement a un en-tête.\n",
    "Quand le fichier taille des Go voir de To de données, cette vérification peut vite devenir coûteuse, c'est pourquoi on préfère traiter les données sans en-tête.\n",
    "\n",
    "Trouvez une solution pour supprimer l'en-tête :\n",
    "\n",
    "Indice: utilisez la method filter de l'API RDD.\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/pyspark.html?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'04104147,4/16/2013 12:00:00 AM,MIDNIGHT,HOMICIDE,KNIFE,8/7/2015 8:34:01 AM,1500 - 1599 BLOCK OF 1ST STREET SW,398943,133729,6,6D,FIRST,105,9,,006400 2,006400,Precinct 127,7/27/2004 8:30:00 PM,7/27/2004 8:30:00 PM'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# indiquer le traitement à effectuer pour supprimer l'en-tête\n",
    "# nous appelerons le RDD sans en-tête \"noHeaderRdd\" \n",
    "\n",
    "header = myRdd.first()\n",
    "noHeaderRdd = myRdd.filter(lambda row : row != header) \n",
    "noHeaderRdd.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sauvegarder les données sans l'entête sous le nom de fichier /data/temporary/wash_dc_crime_incidents_2013_without_header.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "noHeaderRdd.saveAsTextFile('/dataspark/wash_dc_crime_incidents_2013_without_header.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir du fichier sans entête, découper les lignes en cellules individuelles.\n",
    "\n",
    "Nous appelerons le RDD resultant de cette transformation \"CrimeData\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['04104147',\n",
       "  '4/16/2013 12:00:00 AM',\n",
       "  'MIDNIGHT',\n",
       "  'HOMICIDE',\n",
       "  'KNIFE',\n",
       "  '8/7/2015 8:34:01 AM',\n",
       "  '1500 - 1599 BLOCK OF 1ST STREET SW',\n",
       "  '398943',\n",
       "  '133729',\n",
       "  '6',\n",
       "  '6D',\n",
       "  'FIRST',\n",
       "  '105',\n",
       "  '9',\n",
       "  '',\n",
       "  '006400 2',\n",
       "  '006400',\n",
       "  'Precinct 127',\n",
       "  '7/27/2004 8:30:00 PM',\n",
       "  '7/27/2004 8:30:00 PM'],\n",
       " ['05047867',\n",
       "  '6/5/2013 12:00:00 AM',\n",
       "  'MIDNIGHT',\n",
       "  'SEX ABUSE',\n",
       "  'KNIFE',\n",
       "  '8/7/2015 8:32:22 AM',\n",
       "  '6500  - 6599 BLOCK OF PINEY BRANCH ROAD NW',\n",
       "  '397769',\n",
       "  '144596',\n",
       "  '4',\n",
       "  '4B',\n",
       "  'FOURTH',\n",
       "  '402',\n",
       "  '17',\n",
       "  '',\n",
       "  '001901 4',\n",
       "  '001901',\n",
       "  'Precinct 59',\n",
       "  '4/15/2005 12:30:00 PM',\n",
       "  '']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Complétez le code\n",
    "CrimeData= noHeaderRdd.map(lambda x : x.split(','))\n",
    "CrimeData.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La réprésentation des données laisse à désirer sans la schéma des données.\n",
    "En python, on peut associer des noms à chaque valeur du tuple avec le module namedtuple :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CrimeData(date_string='6D', time_string='FIRST', offense='HOMICIDE', latitude='398943', longitude='133729'),\n",
       " CrimeData(date_string='4B', time_string='FOURTH', offense='SEX ABUSE', latitude='397769', longitude='144596'),\n",
       " CrimeData(date_string='1C', time_string='THIRD', offense='SEX ABUSE', latitude='396275', longitude='139402'),\n",
       " CrimeData(date_string='1C', time_string='THIRD', offense='SEX ABUSE', latitude='396518', longitude='139335'),\n",
       " CrimeData(date_string='2A', time_string='SECOND', offense='SEX ABUSE', latitude='395232', longitude='136881'),\n",
       " CrimeData(date_string='5C', time_string='FIFTH', offense='SEX ABUSE', latitude='402158.31', longitude='138824.53'),\n",
       " CrimeData(date_string='7B', time_string='SIXTH', offense='SEX ABUSE', latitude='402837', longitude='133810'),\n",
       " CrimeData(date_string='8D', time_string='SEVENTH', offense='SEX ABUSE', latitude='398794', longitude='127300'),\n",
       " CrimeData(date_string='5E', time_string='FIFTH', offense='HOMICIDE', latitude='400331', longitude='140004'),\n",
       " CrimeData(date_string='7F', time_string='SIXTH', offense='HOMICIDE', latitude='404939', longitude='135095')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "CrimeDataTuple = namedtuple('CrimeData', ['date_string', 'time_string', 'offense', 'latitude', 'longitude'])\n",
    "\n",
    "def map_line(line):\n",
    "  cols = line.split(\",\")\n",
    "  return CrimeDataTuple(date_string=cols[10], time_string=cols[11], offense=cols[3], latitude=cols[7], longitude=cols[8])\n",
    "\n",
    "CrimeDataT = noHeaderRdd.map(map_line)\n",
    "\n",
    "CrimeDataT.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On obtient une liste de tuple plus lisible.\n",
    "\n",
    "A présent, regroupez les données par type de crime (OFFENSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nous utiliserons la methode groupBY \n",
    "\n",
    "# soit on garde une approche par index et donc pas trés lisible\n",
    "# grouped_by_offense_rdd = CrimeData.groupBy(lambda x: x[3])\n",
    "\n",
    "# soit on utiliser les tuples\n",
    "grouped_by_offense_rdd_tuple = CrimeDataT.groupBy(lambda x: x.offense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ASSAULT W/DANGEROUS WEAPON',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x7f5e14b72790>),\n",
       " ('ROBBERY', <pyspark.resultiterable.ResultIterable at 0x7f5e14bceee0>),\n",
       " ('THEFT F/AUTO', <pyspark.resultiterable.ResultIterable at 0x7f5e4047ee50>),\n",
       " ('BURGLARY', <pyspark.resultiterable.ResultIterable at 0x7f5e14bdb4f0>),\n",
       " ('HOMICIDE', <pyspark.resultiterable.ResultIterable at 0x7f5e14baaa30>)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Affichez les 5 premiers éléments :\n",
    "grouped_by_offense_rdd_tuple.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, créez un RDD qui compte le nombre de chaque infraction (OFFENSE). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('HOMICIDE', 104),\n",
       " ('BURGLARY', 3370),\n",
       " ('ROBBERY', 4071),\n",
       " ('THEFT F/AUTO', 10130),\n",
       " ('ASSAULT W/DANGEROUS WEAPON', 2309)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Indice : utilisez la méthode map sur le RDD pair obtenu précédement avec la function len\n",
    "\n",
    "offense_counts = grouped_by_offense_rdd_tuple.map(lambda g: (g[0], len(g[1])))\n",
    "offense_counts.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir du RDD CrimeDataT, répéter l'opération de comptage des OFFENSES avec la méthode RDD groupByKey :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HOMICIDE                       104\n",
      "SEX ABUSE                      299\n",
      "THEFT/OTHER                    12904\n",
      "BURGLARY                       3370\n",
      "ROBBERY                        4071\n",
      "THEFT F/AUTO                   10130\n",
      "ASSAULT W/DANGEROUS WEAPON     2309\n",
      "MOTOR VEHICLE THEFT            2675\n",
      "ARSON                          35\n"
     ]
    }
   ],
   "source": [
    "#Indice : utilisez la méthode map sur le RDD pair obtenu précédement avec la function len\n",
    "\n",
    "offense_counts = CrimeDataT.map(lambda tuple: (tuple.offense, tuple) ).countByKey()\n",
    "\n",
    "for offense, counts in offense_counts.items():\n",
    "    print(\"{0:30s} {1:d}\".format(offense, counts))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combien de meurtres (homicide) y a-t-il eu pendant la période couverte par les données? \n",
    "\n",
    "Attention votre traitement devra être insensible à la casse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('HOMICIDE', 104)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# utilisez le RDD CrimeDataT pour compter le nombre d'homicide\n",
    "# votre traitement devra être insensible à la maniére dont est ecrit homicide\n",
    "# Vous devrez utiliser effectuer le comptage avec la méthode reduceByKey\n",
    "# Vous afficherez le résultat de votre RDD homicide_count\n",
    "\n",
    "homicide_count = CrimeDataT.filter(lambda crime: \"homicide\" in crime.offense.lower()).map(lambda crime: (crime.offense, 1)).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "homicide_count.take(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour un affichage plus sympa :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HOMICIDE 104\n"
     ]
    }
   ],
   "source": [
    "for method, count in homicide_count.collect():\n",
    "    print(\"{0} {1:d}\".format(method, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichez le top 5 des crimes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(12904, 'THEFT/OTHER'),\n",
       " (10130, 'THEFT F/AUTO'),\n",
       " (4071, 'ROBBERY'),\n",
       " (3370, 'BURGLARY'),\n",
       " (2675, 'MOTOR VEHICLE THEFT')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weapon_counts = grouped_by_offense_rdd_tuple.map(lambda g: ( len(g[1]),g[0])).sortByKey(False)\n",
    "\n",
    "weapon_counts.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculer la variance sur le nombre de crime :\n",
    "\n",
    "Indice : vous pourrez utiliser la même approche faite dans le TP MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez à présent fermer le SparkContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
