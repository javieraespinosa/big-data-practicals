{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cet atelier, nous allons explorer certains les concepts de RDD que nous avons abordés. Nous utiliserons un ensemble de données comprenant les crimes signalés à Washington, du 3 octobre 2015 au 2 octobre 2016. Ces données proviennent du catalogue de données ouvertes du district de Columbia. Nous utiliserons ces données pour explorer certaines transformations et actions sur les RDD et les pairs RDD.\n",
    "https://github.com/frankieliu/databricks2/blob/master/wash_dc_crime_incidents_2013.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons à présent commencer le TP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Pré-requis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Démarrer HDFS"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "docker start namenode datanode1 datanode2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Démarrer Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour démarrer votre cluster Spark, si cela n'a pas déjà été fait, vous devrez ouvrir un terminal puis exécuter la commande suivante :"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "docker start smaster sworker1 sworker2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérifiez que les containers smaster, sworker1 et sworker2 sont démarrés :"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connexion à Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etablissez une connexion au cluster Spark :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Attention le driver ne peut instancier qu'un seul SparkContext\n",
    "# Si vous exécuter plusieurs fois cette cellule vous obtiendrez une erreur:\n",
    "# Cannot run multiple SparkContexts at once;\n",
    "# Pensez bien à fermer le Sparkcontext à la fin de chaque TP avec la commande sc.stop()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(\"spark://smaster:7077\", \"TPSPARK\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour vérifier le point d'accés au cluster : \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sc.master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Créer un RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La première étape consiste à charger les données. \n",
    "Exécutez la cellule suivante pour créer un RDD contenant les données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRdd =  sc.textFile(\"/data/tpspark/wash_dc_crime_incidents_2013.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La lecture d'un fichier n'est pas une action et par conséquent aucun traitement sera effectué.\n",
    "\n",
    "Comptez le nombre de ligne contenu dans le RDD `myRDD` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voyons quelques-unes des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichez les 5 premières lignes du RDD \"myRdd\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Super, comme vous pouvez le constater il y a un en-tête. \n",
    "Cet en-tête est fourni au début du fichier, cela signifie qu'une seul partition disposera de l'en-tête.\n",
    "Pour effectuer un traitement homogène sur l'ensemble des données, nous devrons vérifier si le bloc en cours de traitement a un en-tête.\n",
    "Quand le fichier taille des Go voir de To de données, cette vérification peut vite devenir coûteuse, c'est pourquoi on préfère traiter les données sans en-tête.\n",
    "\n",
    "Trouvez une solution pour supprimer l'en-tête :\n",
    "\n",
    "Indice: utilisez la method filter de l'API RDD.\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/pyspark.html?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indiquer le traitement à effectuer pour supprimer l'en-tête\n",
    "# nous appelerons le RDD sans en-tête \"noHeaderRdd\" \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sauvegarder les données sans l'entête sous le nom de fichier /data/temporary/wash_dc_crime_incidents_2013_without_header.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noHeaderRdd.saveAsTextFile('/dataspark/wash_dc_crime_incidents_2013_without_header.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir du fichier sans entête, découper les lignes en cellules individuelles.\n",
    "\n",
    "Nous appelerons le RDD resultant de cette transformation \"CrimeData\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complétez le code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La réprésentation des données laisse à désirer sans la schéma des données.\n",
    "En python, on peut associer des noms à chaque valeur du tuple avec le module namedtuple :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "CrimeDataTuple = namedtuple('CrimeData', ['date_string', 'time_string', 'offense', 'latitude', 'longitude'])\n",
    "\n",
    "def map_line(line):\n",
    "  cols = line.split(\",\")\n",
    "  return CrimeDataTuple(date_string=cols[10], time_string=cols[11], offense=cols[3], latitude=cols[7], longitude=cols[8])\n",
    "\n",
    "CrimeDataT = noHeaderRdd.map(map_line)\n",
    "\n",
    "CrimeDataT.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On obtient une liste de tuple plus lisible.\n",
    "\n",
    "A présent, regroupez les données par type de crime (OFFENSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nous utiliserons la methode groupBY \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Affichez les 5 premiers éléments :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, créez un RDD qui compte le nombre de chaque infraction (OFFENSE). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Indice : utilisez la méthode map sur le RDD pair obtenu précédement avec la function len\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir du RDD CrimeDataT, répéter l'opération de comptage des OFFENSES avec la méthode RDD groupByKey :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Indice : utilisez la méthode map sur le RDD pair obtenu précédement avec la function len\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combien de meurtres (homicide) y a-t-il eu pendant la période couverte par les données? \n",
    "\n",
    "Attention votre traitement devra être insensible à la casse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilisez le RDD CrimeDataT pour compter le nombre d'homicide\n",
    "# votre traitement devra être insensible à la maniére dont est ecrit homicide\n",
    "# Vous devrez utiliser effectuer le comptage avec la méthode reduceByKey\n",
    "# Vous afficherez le résultat de votre RDD homicide_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichez le top 5 des crimes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez à présent fermer le SparkContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
