{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction au Dataframe avec Spark\n",
    "\n",
    "\n",
    "## 1 Présentation\n",
    "\n",
    "Pour faciliter le développement de programme, Spark à partir de la version 2 intégre la notion de Dataframe.\n",
    "\n",
    "Le DataFrame est une couche d'abstraction des RDD, qui présentent les données comme des tables de base de données sans se préoccuper de la taille des données.\n",
    "Les DataFrames offrent de nombreux avantages:\n",
    "* Une syntaxe beaucoup plus simple.\n",
    "* Possibilité d'utiliser SQL directement dans la trame de données.\n",
    "* La parallélisation des traitements dans une architecture distribuée est gérée par Spark.\n",
    "    \n",
    "Si vous avez utilisé R ou même la bibliothèque pandas avec Python, vous connaissez probablement déjà le concept des DataFrames. Même si Spark supporte plusieurs langages il faut savoir que le langage le moins efficace est le python.\n",
    "\n",
    "Vous êtes prêt, jouons un peu avec les dataframes.\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2 Pré-requis\n",
    "\n",
    "### Démarrer Spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour démarrer votre cluster Spark, si cela n'a pas déjà été fait, vous devrez ouvrir un terminal puis exécuter la commande suivante :"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "docker start smaster sworker1 sworker2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérifiez que les containers smaster, sworker1 et sworker2 sont démarrés :"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connexion à Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir de ce notebook, vous établirez une connexion avec le cluster Spark en python.\n",
    "Exécutez la cellule ci-dessous pour vous connecter à votre Cluster Spark  :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# N'oubliez pas de fermer la connexion à la fin du TP\n",
    "# spark.stop()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"spark://smaster:7077\").appName(\"TPDF02\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre objet de connexion 'spark'  dispose d'un ensemble de fonctions pour analyser différentes sources de données : JSON, AVRO, PARQUET, CSV, base de données ...\n",
    "\n",
    "* `csv(path)`\n",
    "* `jdbc(url, table, ..., connectionProperties)`\n",
    "* `json(path)`\n",
    "* `format(source)`\n",
    "* `load(path)`\n",
    "* `orc(path)`\n",
    "* `parquet(path)`\n",
    "* `table(tableName)`\n",
    "* `text(path)`\n",
    "* `textFile(path)`\n",
    "\n",
    "On pourra aussi y retrouver des fonctions pour associer des schémas de données à une source de données ou configurer les options de lectures des données :\n",
    "\n",
    "* `option(key, value)`\n",
    "* `options(map)`\n",
    "* `schema(schema)`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Les fichiers de données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour lire un fichier, il suffit d'utiliser la fonction associée à votre format de fichier:\n",
    "* csv,txt -> load\n",
    "* parquet -> parquet\n",
    "* json -> json \n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv = spark.read.load(\"/data/tpspark/sales_info.csv\", format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans les options de lecture du fichier, nous avons demandé à Spark d'utiliser le séparteur \",\"  et d'inférer le schéma.\n",
    "\n",
    "Pour consulter le schéma évaluer par Spark utilisez la fonction printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- GOOG: string (nullable = true)\n",
      " |-- Sam: string (nullable = true)\n",
      " |-- 200: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_csv.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour voir les fonctions disponibles avec les dataframes consultez la documentation Spark :\n",
    "https://spark.apache.org/docs/latest/api/python/pyspark.sql.html\n",
    "\n",
    "Pour avoir un aperçu des données, utilisez la méthode show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-----+\n",
      "|GOOG|    Sam|  200|\n",
      "+----+-------+-----+\n",
      "|GOOG|Charlie|120.0|\n",
      "|GOOG|  Frank|340.0|\n",
      "|MSFT|   Tina|600.0|\n",
      "|MSFT|    Amy|124.0|\n",
      "|MSFT|Vanessa|243.0|\n",
      "|  FB|   Carl|870.0|\n",
      "|  FB|  Sarah|350.0|\n",
      "|APPL|   John|250.0|\n",
      "|APPL|  Linda|130.0|\n",
      "|APPL|   Mike|750.0|\n",
      "|APPL|  Chris|350.0|\n",
      "+----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_csv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous l'aurez compris Spark utilise un échantillonage des données pour évaluer le schema des données.\n",
    "Il faut l'avouer, c'est bien plus pratique que les RDD.\n",
    "\n",
    "Bien entendu, si le schéma ne vous convient pas vous pouvez lui indiquer le schema que vous souhaitez (renommer les noms des colonnes par exemple)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "# Required for StructField, StringType, IntegerType, etc.\n",
    "\n",
    "\n",
    "fields = [  StructField(\"Compagnie\", StringType(), True),\n",
    "            StructField(\"Personne\", StringType(), True),\n",
    "            StructField(\"Ventes\", FloatType(), False)\n",
    "            ]\n",
    "            \n",
    "csvSchema = StructType(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv_with_my_schema = spark.read.option(\"header\", \"true\").option(\"delimiter\", \",\").schema(csvSchema).csv(\"/data/tpspark/sales_info.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Compagnie: string (nullable = true)\n",
      " |-- Personne: string (nullable = true)\n",
      " |-- Ventes: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_csv_with_my_schema.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------+\n",
      "|Compagnie|Personne|Ventes|\n",
      "+---------+--------+------+\n",
      "|     GOOG| Charlie| 120.0|\n",
      "|     GOOG|   Frank| 340.0|\n",
      "|     MSFT|    Tina| 600.0|\n",
      "|     MSFT|     Amy| 124.0|\n",
      "|     MSFT| Vanessa| 243.0|\n",
      "|       FB|    Carl| 870.0|\n",
      "|       FB|   Sarah| 350.0|\n",
      "|     APPL|    John| 250.0|\n",
      "|     APPL|   Linda| 130.0|\n",
      "|     APPL|    Mike| 750.0|\n",
      "|     APPL|   Chris| 350.0|\n",
      "+---------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_csv_with_my_schema.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les fichiers csv et texte ne sont pas des formats adaptés pour de grosse volumétrie. \n",
    "Une bonne pratique dans les environnements de BIG DATA est de travailler avec des formats binaires pour 2 raisons :\n",
    "* il consomme moins de stockage.\n",
    "* il est plus facile de déplacer des données via le réseau.\n",
    "\n",
    "Dans les écosystèmes HADOOP, les données sont sérialisées sous 2 formes :\n",
    "* Ligne : Avro, Kryo, Protobuff\n",
    "* Column : Parquet, Orc\n",
    "\n",
    "En entreprise, vous trouverez souvent les formats Avro ou parquet pour leur performance.\n",
    "\n",
    "Pour convertir un fichier en parquet c'est aussi simple que ça :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv_with_my_schema.write.format(\"parquet\").save(\"//data/tpspark//sales_info.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00000-662a3bf1-1bab-4666-9fb0-640a67512ba9-c000.snappy.parquet  _SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!$HADOOP_HOME/bin/hdfs dfs -ls /data/tpspark/sales_info.parquet/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez remarquer qu'au format parquet, notre fichier sales_info.parquet est un répertoire contenant des fichiers part-00000, cela vous rappelle-t'il quelque chose ?\n",
    "En effet, pour traiter des données parquet sait que les données vont être distribuées et par conséquent applique le principe de partitionnement. Pour un fichier de plusieurs Go, il est possible d'indiquer la taille des partitions parquet.\n",
    "\n",
    "Bien entendu, la plupart des formats binaires inscrit le schéma des données dans chaque partition.\n",
    "\n",
    "Vérifions cela :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sales_parquet = spark.read.parquet(\"/data/tpspark/sales_info.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Compagnie: string (nullable = true)\n",
      " |-- Personne: string (nullable = true)\n",
      " |-- Ventes: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Sales_parquet.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------+\n",
      "|Compagnie|Personne|Ventes|\n",
      "+---------+--------+------+\n",
      "|     GOOG| Charlie| 120.0|\n",
      "|     GOOG|   Frank| 340.0|\n",
      "|     MSFT|    Tina| 600.0|\n",
      "|     MSFT|     Amy| 124.0|\n",
      "|     MSFT| Vanessa| 243.0|\n",
      "|       FB|    Carl| 870.0|\n",
      "|       FB|   Sarah| 350.0|\n",
      "|     APPL|    John| 250.0|\n",
      "|     APPL|   Linda| 130.0|\n",
      "|     APPL|    Mike| 750.0|\n",
      "|     APPL|   Chris| 350.0|\n",
      "+---------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Sales_parquet.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut facilement convertir d'un format à un autre : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sales_parquet.write.format('json').save('/data/tpspark/sales.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour lire ensuite vos données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sales_json = spark.read.json(\"/data/tpspark/sales.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------+\n",
      "|Compagnie|Personne|Ventes|\n",
      "+---------+--------+------+\n",
      "|     GOOG| Charlie| 120.0|\n",
      "|     GOOG|   Frank| 340.0|\n",
      "|     MSFT|    Tina| 600.0|\n",
      "|     MSFT|     Amy| 124.0|\n",
      "|     MSFT| Vanessa| 243.0|\n",
      "|       FB|    Carl| 870.0|\n",
      "|       FB|   Sarah| 350.0|\n",
      "|     APPL|    John| 250.0|\n",
      "|     APPL|   Linda| 130.0|\n",
      "|     APPL|    Mike| 750.0|\n",
      "|     APPL|   Chris| 350.0|\n",
      "+---------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Sales_json.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Exploiter les DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour manipuler les DataFrames, nous avons un ensemble de méthodes disponibles :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"/data/tpspark/sales_info.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_collect_as_arrow',\n",
       " '_jcols',\n",
       " '_jdf',\n",
       " '_jmap',\n",
       " '_jseq',\n",
       " '_lazy_rdd',\n",
       " '_repr_html_',\n",
       " '_sc',\n",
       " '_schema',\n",
       " '_sort_cols',\n",
       " '_support_repr_html',\n",
       " '_to_corrected_pandas_type',\n",
       " 'agg',\n",
       " 'alias',\n",
       " 'approxQuantile',\n",
       " 'cache',\n",
       " 'checkpoint',\n",
       " 'coalesce',\n",
       " 'colRegex',\n",
       " 'collect',\n",
       " 'columns',\n",
       " 'corr',\n",
       " 'count',\n",
       " 'cov',\n",
       " 'createGlobalTempView',\n",
       " 'createOrReplaceGlobalTempView',\n",
       " 'createOrReplaceTempView',\n",
       " 'createTempView',\n",
       " 'crossJoin',\n",
       " 'crosstab',\n",
       " 'cube',\n",
       " 'describe',\n",
       " 'distinct',\n",
       " 'drop',\n",
       " 'dropDuplicates',\n",
       " 'drop_duplicates',\n",
       " 'dropna',\n",
       " 'dtypes',\n",
       " 'exceptAll',\n",
       " 'explain',\n",
       " 'fillna',\n",
       " 'filter',\n",
       " 'first',\n",
       " 'foreach',\n",
       " 'foreachPartition',\n",
       " 'freqItems',\n",
       " 'groupBy',\n",
       " 'groupby',\n",
       " 'head',\n",
       " 'hint',\n",
       " 'inputFiles',\n",
       " 'intersect',\n",
       " 'intersectAll',\n",
       " 'isLocal',\n",
       " 'isStreaming',\n",
       " 'is_cached',\n",
       " 'join',\n",
       " 'limit',\n",
       " 'localCheckpoint',\n",
       " 'mapInPandas',\n",
       " 'na',\n",
       " 'orderBy',\n",
       " 'persist',\n",
       " 'printSchema',\n",
       " 'randomSplit',\n",
       " 'rdd',\n",
       " 'registerTempTable',\n",
       " 'repartition',\n",
       " 'repartitionByRange',\n",
       " 'replace',\n",
       " 'rollup',\n",
       " 'sameSemantics',\n",
       " 'sample',\n",
       " 'sampleBy',\n",
       " 'schema',\n",
       " 'select',\n",
       " 'selectExpr',\n",
       " 'semanticHash',\n",
       " 'show',\n",
       " 'sort',\n",
       " 'sortWithinPartitions',\n",
       " 'sql_ctx',\n",
       " 'stat',\n",
       " 'storageLevel',\n",
       " 'subtract',\n",
       " 'summary',\n",
       " 'tail',\n",
       " 'take',\n",
       " 'toDF',\n",
       " 'toJSON',\n",
       " 'toLocalIterator',\n",
       " 'toPandas',\n",
       " 'to_koalas',\n",
       " 'to_pandas_on_spark',\n",
       " 'transform',\n",
       " 'union',\n",
       " 'unionAll',\n",
       " 'unionByName',\n",
       " 'unpersist',\n",
       " 'where',\n",
       " 'withColumn',\n",
       " 'withColumnRenamed',\n",
       " 'withWatermark',\n",
       " 'write',\n",
       " 'writeStream',\n",
       " 'writeTo']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour afficher les colonnes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Compagnie', 'Personne', 'Ventes']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Travaillons à présent sur les manipulations que nous pouvons faire :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dfColVentes=df.select('Ventes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dfColVentes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le résultat retourné par le select est un DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|Ventes|\n",
      "+------+\n",
      "| 120.0|\n",
      "| 340.0|\n",
      "| 600.0|\n",
      "| 124.0|\n",
      "| 243.0|\n",
      "| 870.0|\n",
      "| 350.0|\n",
      "| 250.0|\n",
      "| 130.0|\n",
      "| 750.0|\n",
      "| 350.0|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('Ventes').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour selectionner les N premières lignes, on trouvera la fonction head :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Compagnie='GOOG', Personne='Charlie', Ventes=120.0),\n",
       " Row(Compagnie='GOOG', Personne='Frank', Ventes=340.0)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retourne une d'object RowReturns list of Row objects\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour sélectionner multiple colonne :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|Compagnie|Ventes|\n",
      "+---------+------+\n",
      "|     GOOG| 120.0|\n",
      "|     GOOG| 340.0|\n",
      "|     MSFT| 600.0|\n",
      "|     MSFT| 124.0|\n",
      "|     MSFT| 243.0|\n",
      "|       FB| 870.0|\n",
      "|       FB| 350.0|\n",
      "|     APPL| 250.0|\n",
      "|     APPL| 130.0|\n",
      "|     APPL| 750.0|\n",
      "|     APPL| 350.0|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(['Compagnie','Ventes']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|test|Ventes|\n",
      "+----+------+\n",
      "|GOOG| 120.0|\n",
      "|GOOG| 340.0|\n",
      "|MSFT| 600.0|\n",
      "|MSFT| 124.0|\n",
      "|MSFT| 243.0|\n",
      "|  FB| 870.0|\n",
      "|  FB| 350.0|\n",
      "|APPL| 250.0|\n",
      "|APPL| 130.0|\n",
      "|APPL| 750.0|\n",
      "|APPL| 350.0|\n",
      "+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.Compagnie.alias('test'), df.Ventes).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour le renommage d'une colonne on peut utiliser la méthode alias :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|col1| col2|\n",
      "+----+-----+\n",
      "|GOOG|120.0|\n",
      "|GOOG|340.0|\n",
      "|MSFT|600.0|\n",
      "|MSFT|124.0|\n",
      "|MSFT|243.0|\n",
      "|  FB|870.0|\n",
      "|  FB|350.0|\n",
      "|APPL|250.0|\n",
      "|APPL|130.0|\n",
      "|APPL|750.0|\n",
      "|APPL|350.0|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.Compagnie.alias('col1'), df.Ventes.alias('col2')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si on veut créer de nouvelles colonnes à partir d'une colonne existante  :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------+---------+\n",
      "|Compagnie|Personne|Ventes|VentesAdd|\n",
      "+---------+--------+------+---------+\n",
      "|     GOOG| Charlie| 120.0|   1200.0|\n",
      "|     GOOG|   Frank| 340.0|   3400.0|\n",
      "|     MSFT|    Tina| 600.0|   6000.0|\n",
      "|     MSFT|     Amy| 124.0|   1240.0|\n",
      "|     MSFT| Vanessa| 243.0|   2430.0|\n",
      "|       FB|    Carl| 870.0|   8700.0|\n",
      "|       FB|   Sarah| 350.0|   3500.0|\n",
      "|     APPL|    John| 250.0|   2500.0|\n",
      "|     APPL|   Linda| 130.0|   1300.0|\n",
      "|     APPL|    Mike| 750.0|   7500.0|\n",
      "|     APPL|   Chris| 350.0|   3500.0|\n",
      "+---------+--------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('VentesAdd',df['Ventes'] * 10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour renommer une colonne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+\n",
      "|Company|Personne|Ventes|\n",
      "+-------+--------+------+\n",
      "|   GOOG| Charlie| 120.0|\n",
      "|   GOOG|   Frank| 340.0|\n",
      "|   MSFT|    Tina| 600.0|\n",
      "|   MSFT|     Amy| 124.0|\n",
      "|   MSFT| Vanessa| 243.0|\n",
      "|     FB|    Carl| 870.0|\n",
      "|     FB|   Sarah| 350.0|\n",
      "|   APPL|    John| 250.0|\n",
      "|   APPL|   Linda| 130.0|\n",
      "|   APPL|    Mike| 750.0|\n",
      "|   APPL|   Chris| 350.0|\n",
      "+-------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Simple Rename\n",
    "df.withColumnRenamed('Compagnie','Company').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut filtrer sur un critére :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(df.Personne==\"Sam\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour les opérations d'agrégation, on utilisera les fonctions du module pyspark.sql.functions lequel regroupe les fonctions max, min, mean ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "\n",
    "new_df.groupBy('Company').agg(f.max('Vente').alias('Max Vente'), f.min('Vente').alias('Min Vente'), f.mean('Vente').alias('Moy_Vente')).orderBy('Company').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme pour les bases de données relationnelles, on peut faire des jointures entre des sources de données avec différents formats :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "# Création d'un dataframe des localisations des différentes compagnies.\n",
    "# Le code ci-dessous indique comment créer un dataframe manuellement\n",
    "\n",
    "Location = Row(\"Compagnie\", \"location\")\n",
    "location1 = Location('GOOG', 'US')\n",
    "location2 = Location('MSFT', 'EUROPE')\n",
    "location3 = Location('FB', 'ASIA')\n",
    "\n",
    "locationRows =[location1,location2,location3]\n",
    "df_location = spark.createDataFrame(locationRows)\n",
    "type(df_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On affiche le contenu de notre nouveau dataframe :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_location.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On affiche la structure du dataframe :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On réalise la jointure entre le dataframe df listant toutes les ventes des companies et le dataframe df_location précisant la localisation des companies :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.join(df_location, df_location.Compagnie == df.Compagnie, \"left_outer\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilisation du SQL\n",
    "\n",
    "Pour utiliser des requêtes SQL directement sur un DataFrame, vous devrez l'enregistrer dans une vue temporaire:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# La méthode createOrReplaceTempView enregistre \n",
    "# le DataFrame comme une view temporaire\n",
    "df.createOrReplaceTempView(\"sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sql_sales = spark.sql(\"SELECT * FROM sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Compagnie: string, Personne: string, Ventes: float]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------+\n",
      "|Compagnie|Personne|Ventes|\n",
      "+---------+--------+------+\n",
      "|     GOOG| Charlie| 120.0|\n",
      "|     GOOG|   Frank| 340.0|\n",
      "|     MSFT|    Tina| 600.0|\n",
      "|     MSFT|     Amy| 124.0|\n",
      "|     MSFT| Vanessa| 243.0|\n",
      "|       FB|    Carl| 870.0|\n",
      "|       FB|   Sarah| 350.0|\n",
      "|     APPL|    John| 250.0|\n",
      "|     APPL|   Linda| 130.0|\n",
      "|     APPL|    Mike| 750.0|\n",
      "|     APPL|   Chris| 350.0|\n",
      "+---------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_sales.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bien entendu, vous pouvez appliquer des requêtes SQL avec des conditions :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------+\n",
      "|Compagnie|Personne|Ventes|\n",
      "+---------+--------+------+\n",
      "|     MSFT|    Tina| 600.0|\n",
      "|       FB|    Carl| 870.0|\n",
      "|     APPL|    Mike| 750.0|\n",
      "+---------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM sales WHERE Ventes > 500\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark SQL supporte un sous ensemble de la norme SQL92. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bravo !!!\n",
    "\n",
    "Vous êtes prêt à faire vos premiers exercices avec Spark."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "nbpresent": {
   "slides": {},
   "themes": {
    "default": "0535adbc-b74f-46cc-9cd6-4eabe2477c8e",
    "theme": {
     "0535adbc-b74f-46cc-9cd6-4eabe2477c8e": {
      "backgrounds": {
       "backgroundColor": {
        "background-color": "backgroundColor",
        "id": "backgroundColor"
       }
      },
      "id": "0535adbc-b74f-46cc-9cd6-4eabe2477c8e",
      "palette": {
       "backgroundColor": {
        "id": "backgroundColor",
        "rgb": [
         43,
         43,
         43
        ]
       },
       "headingColor": {
        "id": "headingColor",
        "rgb": [
         238,
         238,
         238
        ]
       },
       "linkColor": {
        "id": "linkColor",
        "rgb": [
         19,
         218,
         236
        ]
       },
       "mainColor": {
        "id": "mainColor",
        "rgb": [
         238,
         238,
         238
        ]
       }
      },
      "rules": {
       "a": {
        "color": "linkColor"
       },
       "h1": {
        "color": "headingColor",
        "font-family": "Oswald",
        "font-size": 7
       },
       "h2": {
        "color": "headingColor",
        "font-family": "Oswald",
        "font-size": 5
       },
       "h3": {
        "color": "headingColor",
        "font-family": "Oswald",
        "font-size": 3.75
       },
       "h4": {
        "color": "headingColor",
        "font-family": "Oswald",
        "font-size": 3
       },
       "h5": {
        "color": "headingColor",
        "font-family": "Oswald"
       },
       "h6": {
        "color": "headingColor",
        "font-family": "Oswald"
       },
       "h7": {
        "color": "headingColor",
        "font-family": "Oswald"
       },
       "li": {
        "color": "mainColor",
        "font-family": "Lato",
        "font-size": 5
       },
       "p": {
        "color": "mainColor",
        "font-family": "Lato",
        "font-size": 5
       }
      },
      "text-base": {
       "color": "mainColor",
       "font-family": "Lato",
       "font-size": 5
      }
     },
     "cc59980f-cb69-400a-b63a-1fb85ca73c8a": {
      "backgrounds": {
       "dc7afa04-bf90-40b1-82a5-726e3cff5267": {
        "background-color": "31af15d2-7e15-44c5-ab5e-e04b16a89eff",
        "id": "dc7afa04-bf90-40b1-82a5-726e3cff5267"
       }
      },
      "id": "cc59980f-cb69-400a-b63a-1fb85ca73c8a",
      "palette": {
       "19cc588f-0593-49c9-9f4b-e4d7cc113b1c": {
        "id": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "rgb": [
         252,
         252,
         252
        ]
       },
       "31af15d2-7e15-44c5-ab5e-e04b16a89eff": {
        "id": "31af15d2-7e15-44c5-ab5e-e04b16a89eff",
        "rgb": [
         68,
         68,
         68
        ]
       },
       "50f92c45-a630-455b-aec3-788680ec7410": {
        "id": "50f92c45-a630-455b-aec3-788680ec7410",
        "rgb": [
         197,
         226,
         245
        ]
       },
       "c5cc3653-2ee1-402a-aba2-7caae1da4f6c": {
        "id": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "rgb": [
         43,
         126,
         184
        ]
       },
       "efa7f048-9acb-414c-8b04-a26811511a21": {
        "id": "efa7f048-9acb-414c-8b04-a26811511a21",
        "rgb": [
         25.118061674008803,
         73.60176211453744,
         107.4819383259912
        ]
       }
      },
      "rules": {
       "a": {
        "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c"
       },
       "blockquote": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-size": 3
       },
       "code": {
        "font-family": "Anonymous Pro"
       },
       "h1": {
        "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "font-family": "Merriweather",
        "font-size": 8
       },
       "h2": {
        "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "font-family": "Merriweather",
        "font-size": 6
       },
       "h3": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-family": "Lato",
        "font-size": 5.5
       },
       "h4": {
        "color": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "font-family": "Lato",
        "font-size": 5
       },
       "h5": {
        "font-family": "Lato"
       },
       "h6": {
        "font-family": "Lato"
       },
       "h7": {
        "font-family": "Lato"
       },
       "li": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-size": 3.25
       },
       "pre": {
        "font-family": "Anonymous Pro",
        "font-size": 4
       }
      },
      "text-base": {
       "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
       "font-family": "Lato",
       "font-size": 4
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
